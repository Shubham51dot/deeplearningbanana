{"metadata":{"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from collections import deque\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport random\nimport torch\nfrom unityagents import UnityEnvironment\n%matplotlib inline","metadata":{"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-4e32d456b1ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0munityagents\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUnityEnvironment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"],"ename":"ModuleNotFoundError","evalue":"No module named 'torch'","output_type":"error"}]},{"cell_type":"code","source":"from dqnagent import Agent","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"env = UnityEnvironment(file_name=\"/data/Banana_Linux_NoVis/Banana.x86_64\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"brain_name = env.brain_names[0]\nbrain = env.brains[brain_name]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"env_info = env.reset(train_mode=True)[brain_name]\nprint('Number of agents:', len(env_info.agents))\naction_size = brain.vector_action_space_size\nprint('Number of actions:', action_size) \nstate = env_info.vector_observations[0]\nprint('States look like:', state)\nstate_size = len(state)\nprint('States have length:', state_size)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"env_info = env.reset(train_mode=False)[brain_name] \nstate = env_info.vector_observations[0]           \nscore = 0                                          \nwhile True:\n    action = np.random.randint(action_size)       \n    env_info = env.step(action)[brain_name]        \n    next_state = env_info.vector_observations[0]   \n    reward = env_info.rewards[0]                   \n    done = env_info.local_done[0]                  \n    score += reward                               \n    state = next_state                             \n    if done:                                       \n        break   \nprint(\"Score: {}\".format(score))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dqn(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995, train_mode=True, \n        ckpt_path='pth_checkpoints/checkpoint.pth'):\n    scores = []                        \n    scores_window = deque(maxlen=100) \n    moving_avgs = []                   \n    eps = eps_start                    \n    for i_episode in range(1, n_episodes+1):\n        env_info = env.reset(train_mode=train_mode)[brain_name] \n        state = env_info.vector_observations[0]                \n        score = 0\n        for t in range(max_t):\n            action = agent.act(state, eps)                     \n            env_info = env.step(action)[brain_name]            \n            next_state = env_info.vector_observations[0]        \n            reward = env_info.rewards[0]                       \n            done = env_info.local_done[0]                      \n            agent.step(state, action, reward, next_state, done) \n            score += reward\n            if done:\n                break \n        scores_window.append(score)         \n        scores.append(score)                \n        moving_avg = np.mean(scores_window)  \n        moving_avgs.append(moving_avg)      \n        eps = max(eps_end, eps_decay*eps)    \n        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, moving_avg), end=\"\")\n        if i_episode % 100 == 0:\n            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, moving_avg))\n        if moving_avg >= 13.0:\n            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, moving_avg))\n            if train_mode:\n                torch.save(agent.qnetwork_local.state_dict(), ckpt_path)\n            break\n    return scores, moving_avgs","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"agent = Agent(state_size=state_size, action_size=action_size, seed=0, use_double=False, use_dueling=False)\nscores, avgs = dqn(n_episodes=600, eps_decay=0.98, eps_end=0.02, ckpt_path='pth_checkpoints/v28_checkpoint.pth')\nfig = plt.figure()\nax = fig.add_subplot(111)\nplt.plot(np.arange(len(scores)), scores, label='DQN+RB+FC64')\nplt.plot(np.arange(len(scores)), avgs, c='r', label='average')\nplt.ylabel('Score')\nplt.xlabel('Episode #')\nplt.legend(loc='upper left');\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"agent = Agent(state_size=state_size, action_size=action_size, seed=0)\ncheckpoint = 'pth_checkpoints/v28_checkpoint.pth'\nagent.qnetwork_local.load_state_dict(torch.load(checkpoint))\nnum_episodes = 10\nscores = []\nfor i_episode in range(1,num_episodes+1):\n    env_info = env.reset(train_mode=False)[brain_name] \n    state = env_info.vector_observations[0]            \n    score = 0                                         \n    while True:\n        action = agent.act(state, eps=0)              \n        env_info = env.step(action)[brain_name]       \n        next_state = env_info.vector_observations[0]   \n        reward = env_info.rewards[0]                  \n        done = env_info.local_done[0]                        \n        score += reward                                \n        state = next_state                            \n        if done:                                       \n            scores.append(score)\n            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores)))\n            break\nfig = plt.figure()\nax = fig.add_subplot(111)\nplt.plot(np.arange(len(scores)), scores)\nplt.ylabel('Score')\nplt.xlabel('Episode #')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"env.close()","metadata":{},"execution_count":null,"outputs":[]}]}